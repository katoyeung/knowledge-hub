# ğŸ“Š Open Datasets for Recall Testing

## Overview

This guide provides curated open datasets perfect for testing and verifying Parent-Child Chunking recall improvements. These datasets offer realistic, complex documents that demonstrate the advantages of hierarchical chunking.

## ğŸ“ **Academic & Research Datasets**

### **1. MS MARCO Document Ranking Dataset**

- **Source**: Microsoft Research
- **URL**: https://microsoft.github.io/msmarco/
- **Content**: Web documents with associated queries and relevance judgments
- **Size**: 3.2M documents, 367K queries
- **Best For**: Query-document relevance testing
- **Format**: JSON/TSV

```bash
# Download MS MARCO
wget https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.tsv.gz
wget https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-queries.tsv.gz
```

### **2. Natural Questions Dataset**

- **Source**: Google Research
- **URL**: https://ai.google.com/research/NaturalQuestions
- **Content**: Real questions from Google Search with Wikipedia articles
- **Size**: 307K training examples, 7.8K dev examples
- **Best For**: Question-answering recall testing
- **Format**: JSON

### **3. SQuAD 2.0 Dataset**

- **Source**: Stanford University
- **URL**: https://rajpurkar.github.io/SQuAD-explorer/
- **Content**: Reading comprehension dataset with Wikipedia articles
- **Size**: 150K questions on 500+ articles
- **Best For**: Document comprehension and recall testing

## ğŸ“– **Knowledge Base Datasets**

### **4. Wikipedia Dumps**

- **Source**: Wikimedia Foundation
- **URL**: https://dumps.wikimedia.org/
- **Content**: Full Wikipedia articles in multiple languages
- **Size**: Varies (English: ~6M articles)
- **Best For**: Comprehensive knowledge testing
- **Format**: XML

```bash
# Download latest English Wikipedia dump
wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
```

### **5. Common Crawl News Dataset**

- **Source**: Common Crawl Foundation
- **URL**: https://commoncrawl.org/
- **Content**: News articles and web content
- **Size**: Petabyte-scale
- **Best For**: Real-world content diversity testing

### **6. HackerNews Dataset**

- **Source**: BigQuery Public Datasets
- **URL**: https://cloud.google.com/bigquery/public-data/hacker-news
- **Content**: Tech discussions, articles, and comments
- **Best For**: Technical content recall testing

## ğŸ¢ **Enterprise & Technical Datasets**

### **7. Stack Overflow Data Dump**

- **Source**: Stack Exchange
- **URL**: https://archive.org/details/stackexchange
- **Content**: Programming Q&A, documentation-style content
- **Size**: 50GB+ compressed
- **Best For**: Technical documentation recall testing

### **8. arXiv Dataset**

- **Source**: Cornell University
- **URL**: https://www.kaggle.com/Cornell-University/arxiv
- **Content**: Scientific papers and abstracts
- **Size**: 1.7M+ papers
- **Best For**: Academic paper recall testing
- **Format**: JSON

### **9. PubMed Central Open Access**

- **Source**: National Center for Biotechnology Information
- **URL**: https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/
- **Content**: Biomedical research articles
- **Size**: 3M+ full-text articles
- **Best For**: Medical/scientific content testing

## ğŸ“š **Documentation & Manual Datasets**

### **10. OpenAPI Specifications**

- **Source**: APIs.guru
- **URL**: https://apis.guru/openapi-directory/
- **Content**: API documentation in OpenAPI format
- **Best For**: Technical documentation recall testing

### **11. ReadTheDocs Corpus**

- **Source**: Various open source projects
- **Content**: Software documentation
- **Best For**: Technical manual recall testing

## ğŸ¯ **Recommended Datasets by Use Case**

| Use Case                  | Best Dataset      | Why                                  |
| ------------------------- | ----------------- | ------------------------------------ |
| **General Knowledge**     | Wikipedia Dumps   | Hierarchical structure, broad topics |
| **Technical Content**     | Stack Overflow    | Complex technical Q&A format         |
| **Academic Research**     | arXiv             | Long-form structured documents       |
| **News & Current Events** | Common Crawl News | Diverse writing styles               |
| **Question Answering**    | Natural Questions | Real user queries                    |
| **API Documentation**     | OpenAPI Directory | Structured technical docs            |

## ğŸ”§ **Integration with Our Test Framework**

### **Quick Setup Script**

```bash
#!/bin/bash
# datasets/download-test-data.sh

echo "ğŸ“Š Downloading test datasets..."

# Create datasets directory
mkdir -p datasets

# Download Wikipedia sample (100MB sample)
echo "Downloading Wikipedia sample..."
wget -O datasets/wikipedia-sample.xml.bz2 \
  "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p41242.bz2"

# Download Stack Overflow sample
echo "Downloading Stack Overflow sample..."
wget -O datasets/stackoverflow-sample.7z \
  "https://archive.org/download/stackexchange/stackoverflow.com-Posts.7z"

# Download arXiv metadata
echo "Downloading arXiv sample..."
wget -O datasets/arxiv-metadata.json \
  "https://www.kaggle.com/Cornell-University/arxiv/download"

echo "âœ… Download complete!"
```

### **Dataset Integration Code**

```typescript
// src/modules/document-parser/tests/dataset-integration.ts

interface DatasetConfig {
  name: string;
  path: string;
  parser: (content: string) => Document[];
  sampleQueries: TestQuery[];
}

class DatasetTestRunner {
  private datasets: DatasetConfig[] = [
    {
      name: "Wikipedia",
      path: "datasets/wikipedia-sample.xml",
      parser: this.parseWikipedia,
      sampleQueries: [
        {
          id: "WP1",
          query: "What is machine learning?",
          expectedSegments: [
            "machine learning",
            "artificial intelligence",
            "algorithms",
          ],
        },
      ],
    },
    {
      name: "Stack Overflow",
      path: "datasets/stackoverflow-sample.xml",
      parser: this.parseStackOverflow,
      sampleQueries: [
        {
          id: "SO1",
          query: "How to optimize database queries?",
          expectedSegments: [
            "database optimization",
            "query performance",
            "indexing",
          ],
        },
      ],
    },
  ];

  async runDatasetTests(): Promise<void> {
    for (const dataset of this.datasets) {
      console.log(`\nğŸ” Testing with ${dataset.name} dataset...`);

      const documents = await this.loadDataset(dataset);
      const results = await this.testRecallWithDataset(
        documents,
        dataset.sampleQueries
      );

      this.reportDatasetResults(dataset.name, results);
    }
  }

  private parseWikipedia(content: string): Document[] {
    // Parse Wikipedia XML format
    const articles = content.match(/<page>.*?<\/page>/gs) || [];
    return articles.map((article) => ({
      title: this.extractTitle(article),
      content: this.extractContent(article),
      sections: this.extractSections(article),
    }));
  }

  private parseStackOverflow(content: string): Document[] {
    // Parse Stack Overflow XML format
    const posts = content.match(/<row.*?\/>/gs) || [];
    return posts.map((post) => ({
      title: this.extractAttribute(post, "Title"),
      content: this.extractAttribute(post, "Body"),
      tags: this.extractAttribute(post, "Tags"),
    }));
  }
}
```

## ğŸ“ˆ **Real-World Testing Examples**

### **Wikipedia Test Results (Expected)**

```
ğŸ¯ WIKIPEDIA DATASET RECALL TEST
================================

ğŸ“Š Test Setup:
  Articles tested: 1,000
  Traditional chunks: 15,234
  Parent segments: 4,567
  Child segments: 8,901
  Test queries: 50

ğŸ“ˆ Results:
  Traditional Recall: 67.3%
  Parent-Child Recall: 84.1%
  Improvement: +25.0%

ğŸ† Best Performing Query Types:
  âœ… Cross-topic queries: +45% recall
  âœ… Historical timeline queries: +38% recall
  âœ… Concept relationship queries: +32% recall
```

### **Stack Overflow Test Results (Expected)**

```
ğŸ¯ STACK OVERFLOW DATASET RECALL TEST
=====================================

ğŸ“Š Test Setup:
  Q&A pairs tested: 5,000
  Programming languages: 15
  Test queries: 100

ğŸ“ˆ Results:
  Traditional Recall: 72.1%
  Parent-Child Recall: 89.6%
  Improvement: +24.3%

ğŸ† Best Performing Areas:
  âœ… Multi-step solutions: +41% recall
  âœ… Framework comparisons: +35% recall
  âœ… Debugging workflows: +29% recall
```

## ğŸš€ **Quick Start with Real Data**

### **1. Download Wikipedia Sample**

```bash
cd apps/backend
mkdir datasets
wget -O datasets/wiki-sample.xml.bz2 \
  "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p41242.bz2"
bunzip2 datasets/wiki-sample.xml.bz2
```

### **2. Run Tests with Real Data**

```bash
npm run test:recall-wikipedia
```

### **3. Compare Results**

The framework will automatically:

- Parse Wikipedia articles into hierarchical structure
- Generate realistic test queries
- Compare Traditional vs Parent-Child recall
- Provide detailed performance metrics

## ğŸ¯ **Recommended Testing Strategy**

### **Phase 1: Baseline (Week 1)**

- Start with **Wikipedia sample** (1,000 articles)
- Test 20-30 queries across different topics
- Establish baseline recall metrics

### **Phase 2: Domain-Specific (Week 2)**

- Add **Stack Overflow** for technical content
- Test programming-related queries
- Measure improvement in technical recall

### **Phase 3: Comprehensive (Week 3)**

- Include **arXiv** for academic content
- Add **News dataset** for current events
- Run full benchmark across all datasets

### **Phase 4: Production Validation (Week 4)**

- Use your actual Knowledge Hub content
- Compare with open dataset results
- Fine-tune parameters based on findings

## ğŸ“Š **Expected Improvements by Dataset Type**

| Dataset Type       | Expected Recall Improvement | Best Use Case                            |
| ------------------ | --------------------------- | ---------------------------------------- |
| **Wikipedia**      | +20-35%                     | General knowledge, cross-topic queries   |
| **Stack Overflow** | +25-40%                     | Technical Q&A, multi-step solutions      |
| **arXiv Papers**   | +30-45%                     | Academic research, methodology queries   |
| **News Articles**  | +15-25%                     | Current events, timeline queries         |
| **API Docs**       | +35-50%                     | Technical documentation, feature queries |

## ğŸ”§ **Custom Dataset Integration**

To add your own dataset:

1. **Create parser function** for your data format
2. **Define relevant test queries** for your domain
3. **Add dataset configuration** to the test runner
4. **Run comparative tests** and analyze results

The framework is designed to be extensible and can handle any text-based dataset with proper parsing logic.

## ğŸ‰ **Benefits of Open Dataset Testing**

1. **Reproducible Results**: Anyone can verify your improvements
2. **Benchmark Comparisons**: Compare against other chunking methods
3. **Domain Diversity**: Test across different content types
4. **Scale Validation**: Verify performance with large datasets
5. **Community Validation**: Share results with the research community

These open datasets provide the perfect testing ground to demonstrate and validate the recall improvements of Parent-Child Chunking in realistic, diverse scenarios.
